{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 -  Features Numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos:\n",
    "\n",
    "O objetivo desse desafio nesta primeira etapa é analisar os dados do case e estruturar uma Feature Engineering básica apenas com os dados numéricos existentes, sem transformar ou combinar features ou mesmo adicionar informações externas. \n",
    "\n",
    "Ao final do desafio, será treinado um modelo de regressão linear com as features obtidas. Esse modelo será testado contra uma massa de teste, separada previamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup do Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Functions do Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports de Libs Externas (padrão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports de Libs Locais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_california_housing_prices\n",
    "from pipeline import NumericalFeaturesImputer, LogFeaturesTransform, FeaturesChoiceTransform\n",
    "from utils import calculate_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = load_california_housing_prices()\n",
    "x_train = dataset[\"train\"][\"x\"]\n",
    "y_train = dataset[\"train\"][\"y\"]\n",
    "x_test = dataset[\"test\"][\"x\"]\n",
    "y_test = dataset[\"test\"][\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering c/ Features Numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features numéricas são as primeiras a serem tratadas, por serem as mais fáceis de compreender e de relacionar com o problema. As seções a seguir focam no tratamento e limpeza dessas features, o primeiro passo na engenharia de features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuição das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    \"longitude\", \"latitude\", \n",
    "    \"housing_median_age\", \n",
    "    \"total_rooms\", \"total_bedrooms\", \n",
    "    \"population\", \"households\", \"median_income\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrevendo as distribuições\n",
    "cuts = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "x_train[numerical_cols].describe(percentiles=cuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tomada de decisão, é sempre mais informativo usar visualizações. Com elas é possível observar o formato da distribuição (normal, exponencial, logarítmica, etc) e os outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando as distribuições com histogramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = x_train[numerical_cols].hist(bins=50, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção e Tratamento de Nulos\n",
    "\n",
    "Conhecendo a distribuição das features, já é possível traçar estratégias de tratamento de valores nulos. A primeira tarefa é detectar a proporção de nulos nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Verificar quantidade de Nulos em cada feature\n",
    "* Entender que Nulos podem aparecer também nos dados de teste, portanto apenas descartar o dado não resolve\n",
    "* Entender que medida (média ou mediana) deve ser imputada em cada feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_null = x_train[numerical_cols].isnull()\n",
    "null_data = pd.DataFrame({\n",
    "    \"count\": x_null.sum(),\n",
    "    \"mean\": x_null.mean()\n",
    "})\n",
    "null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente a ocorrência de valores nulos ocorre por algum ruído no processo de obtenção de dados, o que significa que muito provavelmente eles ocorrerão também em dados de produção.\n",
    "\n",
    "Existem algumas técnicas de tratamento de valores nulos:\n",
    "\n",
    "- Atribuir um valor padrão fora da distribuição;\n",
    "- Criar modelos para inferir os valores a partir das outras features;\n",
    "- Imputar um valor referente à distribuição:\n",
    "    - média\n",
    "    - mediana\n",
    "    \n",
    "Para features numéricas em que conhecemos a distribuição mas não foram tratados os outliers, o mais recomendado é utilizar a **mediana**, cujo valor é pouco afetado por outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "#### Tarefa (1.1) \n",
    "\n",
    "Completar a implementação do transformador de dados `NumericalFeaturesImputer` usando a `mediana`. \n",
    "\n",
    "A classe está no arquivo `pipeline.py`.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se observar a seguir uma amostra dos dados originais, i.e. antes da transformação do imputer. É interessante notar os valores de `NaN` nas colunas `population` e `median_income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x_tr = x_train.head(10).append(x_train.tail(10))\n",
    "sample_x_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina-se o `imputer` como se fosse um modelo, usando a massa de teino. A aplicação do `imputer` treinado sobre a amostra está mostrada a seguir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_imputer = NumericalFeaturesImputer(numerical_cols)\n",
    "numerical_imputer.fit(x_train)\n",
    "numerical_imputer.transform(sample_x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando o `imputer` em toda a massa de treino, pode-se ver o efeito sobre a ocorrência de `NULLs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_null = numerical_imputer.transform(x_train)[numerical_cols].isnull()\n",
    "null_data = pd.DataFrame({\n",
    "    \"count\": x_null.sum(),\n",
    "    \"mean\": x_null.mean()\n",
    "})\n",
    "null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para as próximas etapas, será necessário que a massa de treino seja transformada com o `imputer` criado, pois assim não se propaga o problema de haverem valores faltantes na base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = numerical_imputer.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação Logarítmica de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os histogramas, já fica evidente que a distribuição de algumas features é **exponencial**. A análise de outliers desse tipo de distribuição pode ser prejudicada pela alta concentração de elementos em uma pequena parte do domínio. \n",
    "\n",
    "Uma forma de corrigir essa distorção é transformar esses dados com a função **logarítmica**; a transformação com essa função torna a distribuição das features mais próxima da normal. Por isso, serão transformadas apenas as features que possuem a distribuição exponencial bem evidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_cols = [\"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda é cedo para descartar as features originais tratadas, pois elas podem ter ainda algum poder preditivo que pode ficar ocluso pela transformação. O descarte de features normalmente é feito quando se detecta multi-colinearidade entre as features ou durante uma etapa de _feature selection_.\n",
    "\n",
    "Por hora, as novas features serão integradas ao dataset original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "#### Tarefa (1.2) \n",
    "\n",
    "Completar a implementação do transformador de dados `LogFeaturesTransform`. \n",
    "\n",
    "A classe está no arquivo `pipeline.py`.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = LogFeaturesTransform(log_cols).fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_cols = [f\"log_of_{c}\" for c in log_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = x_train[new_cols].hist(bins=50, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção e Remoção de Outliers\n",
    "\n",
    "Outliers podem deformar a percepção do domínio para o aprendizado de um modelo linear, impedindo o mesmo de encontrar uma solução correta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando os Box Plots para observar os Outliers\n",
    "\n",
    "Uma maneira de se estudar os outliers para definir cortes é usando o gráfico `BoxPlot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "_ = sns.boxplot(x=\"value\", y=\"variable\", data=x_train[new_cols].melt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando os cortes:\n",
    "\n",
    "Os cortes devem ser aplicados na massa de treino, para que apenas os dados dentro da distribuição correta sejam usados para o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "#### Tarefa (1.3)\n",
    "\n",
    "Usando as distribuições de dados vistas anteriormente, escolher `features` e cortes de mínimo e máximo para completar a implementação dda função `remove_outliers`. \n",
    "\n",
    "A classe está no arquivo `utils.py`.\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na tabela a seguir pode-se observar quais cortes serão aplicados na massa de treino e qual o percentual dos dados que é perdido no processo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_index, cuts_table = calculate_outliers(x_train)\n",
    "ori_size = keep_index.shape[0]\n",
    "new_size = keep_index.sum()\n",
    "\n",
    "print(f\"Size of 'x_train' before Cuts:\\t {ori_size}\")\n",
    "print(f\"Size of 'x_train'  after Cuts:\\t {new_size} (-{100. * (1. - keep_index.mean()): 0.2f} %)\")\n",
    "cuts_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os índices calculados devem ser guardados para uso posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(\n",
    "    value=keep_index,\n",
    "    filename=os.path.join(\"pipelines\", \"keep_index.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação de um Modelo Linear\n",
    "\n",
    "O modelo de machine learning é apenas a parte final de um pipeline de processamentos; o pipeline completo é formado por todas as etapas de pré-processamento desde o dado bruto até as etapas de normalização e redução de dimensionalidade, finalizado pelo modelo preditivo.\n",
    "\n",
    "O framework `Scikit-Learn` implementa uma ferramenta que permite a montagem de um pipeline completo, que pode ser treinado e usado para predição como um objeto único, que pode ser inclusive salvo em um arquivo. Isso permite que todo o pipeline possa ser exportado para produção sem ser reimplementado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reload das massas de Treino e de Teste\n",
    "\n",
    "As massas de dados de Treino e de Teste serão carregadas novamente para que seja aplicado o pipeline de pré-processamento em ambos desde o princípio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = load_california_housing_prices()\n",
    "x_train = dataset[\"train\"][\"x\"]\n",
    "y_train = dataset[\"train\"][\"y\"]\n",
    "x_test = dataset[\"test\"][\"x\"]\n",
    "y_test = dataset[\"test\"][\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deve-se remover os outliers da massa de treino usando a função construída para isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_index = joblib.load(os.path.join(\"pipelines\", \"keep_index.pkl\"))\n",
    "x_train = x_train[keep_index]\n",
    "y_train = y_train[keep_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pipeline de Regressão\n",
    "\n",
    "A construção do pipeline deve incluir:\n",
    "\n",
    "* Todas as etapas de pré-processamento\n",
    "* Seleção de Features\n",
    "* Normalização (Z-Score)\n",
    "* Modelo Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pipeline de Pré-Processamento\n",
    "\n",
    "Apenas as etapas da Feature Engineering Numérica devem estar aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"longitude\", \"latitude\", \n",
    "    \"housing_median_age\", \n",
    "    \"total_rooms\", \"total_bedrooms\",  \n",
    "    \"population\", \"households\", \"median_income\"\n",
    "]\n",
    "\n",
    "log_transform_features = [\n",
    "    \"total_rooms\", \"total_bedrooms\", \n",
    "    \"population\", \"households\", \n",
    "    \"median_income\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_eng_pipeline = Pipeline([\n",
    "    (\"numerical_imputer\",          NumericalFeaturesImputer(numerical_features)),\n",
    "    (\"logarithmic_transform\",      LogFeaturesTransform(log_transform_features))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse pipeline será treinado e salvo para ser usado nos próximos notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_pipeline.fit(x_train)\n",
    "joblib.dump(\n",
    "    value=feat_eng_pipeline.fit(x_train),\n",
    "    filename=os.path.join(\"pipelines\", \"numerical_feat_eng.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pipeline Completo\n",
    "\n",
    "Todas as etapas de pré-processamento devem estar incluídas nesse pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_features = numerical_features + [f\"log_of_{c}\" for c in log_transform_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"numerical_feat_eng\",         feat_eng_pipeline),\n",
    "    (\"features_choice\",            FeaturesChoiceTransform(chosen_features)),\n",
    "    (\"zscore\",                     StandardScaler()),\n",
    "    (\"predictor\",                  ElasticNet()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Treinar e avaliar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento de todo o pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliação do modelo nas massas de **treino** e de **teste**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = y_train\n",
    "y_pred = pipeline.predict(x_train)\n",
    "mse_tr = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "r2_tr = r2_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = pipeline.predict(x_test)\n",
    "mse_te = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "r2_te = r2_score(y_true=y_true, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    index=[\"train\", \"test\"],\n",
    "    columns=[\"MSE\", \"R^2\"],\n",
    "    data=[\n",
    "        [mse_tr, r2_tr],\n",
    "        [mse_te, r2_te]\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
