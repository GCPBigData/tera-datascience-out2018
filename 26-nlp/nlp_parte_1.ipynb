{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula #26 – Processamento de Linguagem Natural & Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "NLP (ou _Natural Language Processing_ - a sigla em português é PLN, _Processamento de Linguagem Natural_, mas essa sigla também é usada para falar de _Programação Neurolinguística_, então, vamos continuar usando NLP, ok?) é a área relacionada a técnicas para o entendimento de linguagem humana (a linguagem natural). A aplicação de técnicas de NLP tem sido vista em muitos domínios, desde a correção de palavras e sugestão de palavras na barra de busca de sites como o _Google_, até em sistemas mais \"sofisticados\" como os tradutores automáticos e os _home assistant_ e _smart assistants_, como Siri, Alexa ou Google Home, que podem auxiliar na execução de certas tarefas, como agendar compromissos, embora eles ainda [estejam longe de ser à prova de erros](https://www.nytimes.com/2018/05/25/business/amazon-alexa-conversation-shared-echo.html). Com o crescimento de conteúdo (mais artigos e mais fontes espalhadas), a importância do NLP também tem crescido, pois tarefas como sumarização automática e classificação automática de conteúdo (por ex., para checagem contra _fake news_) são cada vez mais necessárias em nossas vidas.\n",
    "\n",
    "Nesta aula, vamos focar no uso de algumas técnicas de NLP para processamento de texto. Entretanto, é importante lembrar que NLP não se restringe a textos escritos, podendo ser aplicado também para processamento de fala, como é o caso dos _smart assistants_.\n",
    "\n",
    "Diferentes tarefas de NLP, em geral, têm como passos iniciais as seguintes etapas:\n",
    "\n",
    "* pré-processamento do texto (que pode ser uma combinação de diferentes processamentos, envolvendo modificações a nível de palavra e identificação de entidades/funções sintáticas/etc.)\n",
    "\n",
    "* transformação do texto em quantidades numéricas (tipicamente vetores de números inteiros ou reais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda de hoje\n",
    "\n",
    "* análise de sentimento (detecção de polaridade)\n",
    "\n",
    "* ferramenta para encontrar ingredientes que combinam (usando _embeddings_ criados por _word2vec_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de sentimento\n",
    "\n",
    "Esse conjunto de dados é composto de reviews de restaurantes e foi modificado a partir do dataset usado no workshop `SemEval` (_International Workshop on Semantic  Evaluation_) na [edição de 2016](http://alt.qcri.org/semeval2016/task5/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/datasets/restaurants_semeval2016/train.csv')\n",
    "test_df = pd.read_csv('data/datasets/restaurants_semeval2016/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It is nearly impossible to get a table, so if ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I won't go back unless someone else is footing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are so many better places to visit!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This place is a must visit!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but the service was a bit slow.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity\n",
       "0  It is nearly impossible to get a table, so if ...  positive\n",
       "1  I won't go back unless someone else is footing...  negative\n",
       "2          There are so many better places to visit!  negative\n",
       "3                        This place is a must visit!  positive\n",
       "4                    but the service was a bit slow.  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I loved it and would HIGHLY RECOMMEND.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nice job!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Try the lobster teriyaki and the rose special ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can’t believe how an expensive NYC restaurant ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  polarity\n",
       "0  To be completely fair, the only redeeming fact...  negative\n",
       "1             I loved it and would HIGHLY RECOMMEND.  positive\n",
       "2                                          Nice job!  positive\n",
       "3  Try the lobster teriyaki and the rose special ...  positive\n",
       "4  Can’t believe how an expensive NYC restaurant ...  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, tanto no dataset de treino, como no de teste, temos o texto e polaridade da sentença. Nosso objetivo é construir um **classificador de sentimentos**, que recebe uma sentença (referente a um review de restaurante) e é capaz de predizer se o review é positivo ou negativo em relação ao restaurante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mussels were fantastic and so was the dessert...definitely going to be back very soon.\n",
      "I got hair in my food 2 times of then !\n",
      "I loved it and would go again.\n",
      "Would NEVER go back\n",
      "A guaranteeed delight!\n",
      "Definitely has one of the best jukebox's i've seen in a long long time.\n",
      "Overall, decent food at a good price, with friendly people.\n",
      "I will definetly be going back.\n",
      "The meat is fresh, the sauces are great, you get kimchi and a salad free with your meal and service is good too.\n",
      "Highly recommended.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(train_df.sample(n=10)['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos exemplos de frases acima, podemos ver que as sentenças incluem pontuações, contrações (e.g. `you're` ao invés de `you are`), letras maiúsculas e minúsculas... Seria ideal que conseguíssemos _normalizar_ o texto, de forma a diminuir a quantidade de palavras diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um módulo bastante utilizado nesse tipo de tarefa é o `spacy`. No `spacy`, é possível treinar um modelo para que ele reconheça certas estruturas comuns em textos de uma determinada língua. No caso do inglês (e também do português), podemos baixar o modelo e começar a usar!\n",
    "\n",
    "Além de ser bem completo, ele costuma ser mais rápido que o módulo \"concorrente\" `nltk`. O [benchmark abaixo](https://blog.thedataincubator.com/2016/04/nltk-vs-spacy-natural-language-processing-in-python/) compara três tarefas (tokenização de palavras, tokenização de sentenças e classificação gramatical - comumente chamado de PoS tag, _part-of-speech tag_). Note que o tempo está em `ms`. Para processar o texto de nosso dataset, não vamos fazer uso do tokenizador de sentenças, logo, para o nosso caso de uso, realmente o `spacy` parece ser a melhor opção!\n",
    "\n",
    "<img src=\"data/nb_figs/timing_nltk_spacy_2016.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando o modelo de inglês\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o modelo\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usando o spacy**\n",
    "\n",
    "Ao fazer uma chamada do tipo `nlp(text)`, é aplicado o pipeline composto pelo `tagger` (PoS tag), `parser` (_parseador_ de dependências) e `ner` (reconhecimento de entidades) no texto. O retorno é um objeto do tipo `Doc`, que é uma sequência de objetos `Token`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"It's also attached to Angel's Share, which is a cool, more romantic bar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[It,\n",
       " 's,\n",
       " also,\n",
       " attached,\n",
       " to,\n",
       " Angel,\n",
       " 's,\n",
       " Share,\n",
       " ,,\n",
       " which,\n",
       " is,\n",
       " a,\n",
       " cool,\n",
       " ,,\n",
       " more,\n",
       " romantic,\n",
       " bar,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entidades (named entities)\n",
    "\n",
    "São termos que se referem a objetos como pessoas, locais, organizações (ex. empresas) etc.\n",
    "\n",
    "Os tipos de entidades disponíveis no modelo do `spacy` são:\n",
    "\n",
    "<img src=\"data/nb_figs/entities_table.png\" width=\"500\"/>\n",
    "\n",
    "Dado um objeto do tipo `Doc`, podemos ver quais são as entidades presentes no texto que ele representa, usando o atributo `ents`, como pode ser visto abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Angel's Share,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que também podemos visualizar as entidades utilizando a ferramenta `displacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">It's also attached to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Angel's Share\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", which is a cool, more romantic bar...</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, nesse caso, houve a identificação (correta) de `Angel's Share` como uma entidade. Identificar as entidades pode ser útil pois, dependendo do problema, remover entidades do texto pode melhorar o desempenho do seu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para as manipulações que queremos fazer, é útil saber um pouco sobre a API do `spacy` para objeto `Token`. Abaixo, temos um excerto selecionado dos métodos e atributos definidos para `Token`.\n",
    "\n",
    "```python\n",
    "Help on Token object:\n",
    "\n",
    "class Token(builtins.object)\n",
    " |  An individual token – i.e. a word, punctuation symbol, whitespace,\n",
    " |  etc.\n",
    " |  \n",
    " |  Methods defined here:\n",
    " |  \n",
    " |  __len__(...)\n",
    " |      The number of unicode characters in the token, i.e. `token.text`.\n",
    " |      \n",
    " |      RETURNS (int): The number of unicode characters in the token.\n",
    " |  \n",
    " |  __str__(self, /)\n",
    " |      Return str(self).\n",
    "\n",
    " |  Data descriptors defined here:\n",
    " |  \n",
    " |  idx\n",
    " |      RETURNS (int): The character offset of the token within the parent\n",
    " |      document.\n",
    " |  \n",
    " |  is_alpha\n",
    " |      RETURNS (bool): Whether the token consists of alpha characters.\n",
    " |      Equivalent to `token.text.isalpha()`.\n",
    " |  \n",
    " |  is_ascii\n",
    " |      RETURNS (bool): Whether the token consists of ASCII characters.\n",
    " |      Equivalent to `[any(ord(c) >= 128 for c in token.text)]`.\n",
    " |  \n",
    " |  is_bracket\n",
    " |      RETURNS (bool): Whether the token is a bracket.\n",
    " |  \n",
    " |  is_currency\n",
    " |      RETURNS (bool): Whether the token is a currency symbol.\n",
    " |  \n",
    " |  is_digit\n",
    " |      RETURNS (bool): Whether the token consists of digits. Equivalent to\n",
    " |      `token.text.isdigit()`.\n",
    " |  \n",
    " |  is_lower\n",
    " |      RETURNS (bool): Whether the token is in lowercase. Equivalent to\n",
    " |      `token.text.islower()`.\n",
    " |  \n",
    " |  is_punct\n",
    " |      RETURNS (bool): Whether the token is punctuation.\n",
    " |  \n",
    " |  is_space\n",
    " |      RETURNS (bool): Whether the token consists of whitespace characters.\n",
    " |      Equivalent to `token.text.isspace()`.\n",
    " |  \n",
    " |  is_stop\n",
    " |      RETURNS (bool): Whether the token is a stop word, i.e. part of a\n",
    " |      \"stop list\" defined by the language data.\n",
    " |  \n",
    " |  is_upper\n",
    " |      RETURNS (bool): Whether the token is in uppercase. Equivalent to\n",
    " |      `token.text.isupper()`\n",
    " |  \n",
    " |  lemma_\n",
    " |      RETURNS (unicode): The token lemma, i.e. the base form of the word,\n",
    " |      with no inflectional suffixes.\n",
    " |  \n",
    " |  like_email\n",
    " |      RETURNS (bool): Whether the token resembles an email address.\n",
    " |  \n",
    " |  like_num\n",
    " |      RETURNS (bool): Whether the token resembles a number, e.g. \"10.9\",\n",
    " |      \"10\", \"ten\", etc.\n",
    " |  \n",
    " |  like_url\n",
    " |      RETURNS (bool): Whether the token resembles a URL.\n",
    " |\n",
    " |  lower_\n",
    " |      RETURNS (unicode): The lowercase token text. Equivalent to\n",
    " |      `Token.text.lower()`.\n",
    " |  \n",
    " |  norm_\n",
    " |      RETURNS (unicode): The token's norm, i.e. a normalised form of the\n",
    " |      token text. Usually set in the language's tokenizer exceptions or\n",
    " |      norm exceptions.\n",
    " |  \n",
    " |  pos_\n",
    " |      RETURNS (unicode): Coarse-grained part-of-speech tag.\n",
    " |  \n",
    " |  sentiment\n",
    " |      RETURNS (float): A scalar value indicating the positivity or\n",
    " |      negativity of the token.\n",
    " |  \n",
    " |  shape_\n",
    " |      RETURNS (unicode): Transform of the tokens's string, to show\n",
    " |      orthographic features. For example, \"Xxxx\" or \"dd\".\n",
    " |  \n",
    " |  string\n",
    " |      Deprecated: Use Token.text_with_ws instead.\n",
    " |  \n",
    " |  subtree\n",
    " |      A sequence of all the token's syntactic descendents.\n",
    " |      \n",
    " |      YIELDS (Token): A descendent token such that\n",
    " |          `self.is_ancestor(descendent)`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabendo mais sobre a API, vemos que é possível extrair atributos dos tokens, como demonstrado abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Explore a API e imprima alguns atributos do objeto Token para entender seu funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra: \"It\" - (é pontuação: \"False\")\n",
      "Palavra: \"'s\" - (é pontuação: \"False\")\n",
      "Palavra: \"also\" - (é pontuação: \"False\")\n",
      "Palavra: \"attached\" - (é pontuação: \"False\")\n",
      "Palavra: \"to\" - (é pontuação: \"False\")\n",
      "Palavra: \"Angel\" - (é pontuação: \"False\")\n",
      "Palavra: \"'s\" - (é pontuação: \"False\")\n",
      "Palavra: \"Share\" - (é pontuação: \"False\")\n",
      "Palavra: \",\" - (é pontuação: \"True\")\n",
      "Palavra: \"which\" - (é pontuação: \"False\")\n",
      "Palavra: \"is\" - (é pontuação: \"False\")\n",
      "Palavra: \"a\" - (é pontuação: \"False\")\n",
      "Palavra: \"cool\" - (é pontuação: \"False\")\n",
      "Palavra: \",\" - (é pontuação: \"True\")\n",
      "Palavra: \"more\" - (é pontuação: \"False\")\n",
      "Palavra: \"romantic\" - (é pontuação: \"False\")\n",
      "Palavra: \"bar\" - (é pontuação: \"False\")\n",
      "Palavra: \"...\" - (é pontuação: \"True\")\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'Palavra: \"{token}\" - (é pontuação: \"{token.is_punct}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Para muitas tarefas de NLP, é bom prestar atenção nas chamadas _stopwords_, que são as palavras mais comuns que aparecem no texto.\n",
    "\n",
    "Como assim? Vamos tokenizar (de maneira simplista, mas note que estamos usando expressões regulares. Para refrescar a memória, você pode usar o [pythex](https://pythex.org/)) e imprimir as palavras mais comuns da coluna `text` de nosso dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 309),\n",
       " ('the', 177),\n",
       " ('a', 103),\n",
       " ('to', 97),\n",
       " ('and', 92),\n",
       " ('i', 90),\n",
       " ('is', 79),\n",
       " ('it', 75),\n",
       " ('for', 56),\n",
       " ('you', 55)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(sum(train_df['text'].str.lower().str.split(r'[\\W\\s]+').tolist(), [])).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vendo a frequência das palavras no texto, podemos entender de que tipo de palavras estamos falando, certo?\n",
    "\n",
    "No modelo que temos carregado, podemos obter a lista de stopwords percorrendo o vocabulário do modelo (`nlp.vocab`) e incluindo na lista somente as palavras que são stopwords (ou seja, com `is_stop` igual a `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = sorted([tok.text for tok in nlp.vocab if tok.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Ponto importante_:** Como nossa tarefa é uma análise de sentimentos, seria muito ruim perder certas _stopwords_ como palavras de negação, afinal, \"Eu **não** gosto disso\"  é muito diferente de \"Eu gosto disso\"!\n",
    "\n",
    "Para facilitar marcar as palavras que gostaríamos de manter no texto (excluindo-as da lista de palavras do modelo do `spacy`), vamos utilizar `widgets`, a partir do módulo [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/index.html). Os `widgets` servem para prover interatividade dentro do notebook, mas vamos logo entender o poder deles com o exemplo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, HBox, VBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkbox = lambda desc, default_value: widgets.Checkbox(\n",
    "    value=default_value,\n",
    "    description=desc,\n",
    "    disabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded = ['cannot', 'never', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'nowhere', 'off']\n",
    "items = [checkbox(w, w in set(excluded)) for w in en_stopwords]\n",
    "all_boxes = []\n",
    "for chunk_items in chunks(items, 7):\n",
    "    all_boxes.append(HBox(chunk_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Marque os boxes das palavras que deveríamos manter no texto (ou seja, que NÃO deveriam ser _stopwords_).\n",
    "\n",
    "Note que já há algumas palvras marcadas. Fique à vontade para marcar mais ou mesmo desmarcar os boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b887428ff6ac4d189996cd6ec61db3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Checkbox(value=False, description='a'), Checkbox(value=False, description='about…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox(all_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rode a célula abaixo para efetivar as mudanças no modelo do spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(en_stopwords):\n",
    "    nlp.vocab[w].is_stop = not items[i].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E vamos checar, por amostragem, se está tudo certo mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert((nlp.vocab['no'].is_stop, nlp.vocab['which'].is_stop) == (False, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluídas da lista de stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cannot',\n",
       " 'never',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'nowhere',\n",
       " 'off']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in en_stopwords if not nlp.vocab[w].is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização do texto\n",
    "\n",
    "Para normalizar os textos de nosso dataset, vamos processar o texto da seguinte forma:\n",
    "\n",
    "* expandir as contrações (e.g. `don't` -> `do not`);\n",
    "\n",
    "* remover entidades;\n",
    "\n",
    "* colocar o texto em letra minúscula (poderia ser em letra maiúscula também, o importante é a consistência!);\n",
    "\n",
    "* remover pontuação;\n",
    "\n",
    "* remover _stopwords_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.contractions import CONTRACTIONS_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTIONS_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa**: Crie uma função chamada `expand_contractions` que recebe uma variável do tipo `string` e expande as contrações que eventualmente existirem lá.\n",
    "\n",
    "Dica: use o dicionário de contrações/expansões `CONTRACTIONS_DICT` e expressões regulares para fazer as substituições. Note que não é necessário que seja mantido o _case_ (a caixa alta/baixa) das palavras, pois vamos colocar todas as palavras em _lowercase_ (caixa baixa). É possível manter o _case_ das palavras e se você estiver curioso sobre isso, sugiro ler mais no [StackOverflow](https://stackoverflow.com/questions/24893977/whats-the-best-way-to-regex-replace-a-string-in-python-but-keep-its-case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    ### complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(expand_contractions(\"It's\").lower() == 'it is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Crie uma função chamada `normalize_text` que recebe uma variável do tipo `string` e retorna uma `string` que:\n",
    "    1. tem as contrações expandidas;\n",
    "    2. não possui entidades, pontuação e _stopwords_;\n",
    "    3. está em letra minúscula.\n",
    "    \n",
    "Dica: use a função criada acima para expandir as contrações e use o modelo do spacy carregado `nlp` para te ajudar a fazer as demais normalizações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Ponto importante_:** há um _bug_ na versão `2.0.0` do modelo do spacy (que é a que estamos usando), que faz com que a detecção de _stopwords_ seja _case sensitive_ (ou seja, dependa do estado da palavra, em caixa alta/baixa). Esse _bug_ faz com que somente sejam reconhecidas como _stopwords_ palavras em _lowercase_ (caixa baixa). Para ler mais sobre isso, você pode ler os comentários sobre isso no [github](https://github.com/explosion/spaCy/issues/1889).\n",
    "\n",
    "Como já foi reforçado antes, em nosso caso, não vamos manter o _case_ das palavras, por isso, para corrigir o comportamento de detecção de _stopwords_, basta que façamos a transformação para _lowercase_ ANTES de checar se a palavra é uma _stopword_, ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ents(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        text = text.replace(ent.text, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = remove_ents(text)\n",
    "    ### complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(normalize_text(\"It's also attached to Angel's Share, which is a cool, more romantic bar...\") == 'attached cool more romantic bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Aplique sua função `normalize_text` à coluna `text` (dos dois dataframes, `train_df` e `test_df`), criando uma coluna nova `norm_text`, que será usada para treinarmos um modelo de análise de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['norm_text'] = ### complete\n",
    "test_df['norm_text'] = ### complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.norm_text.str.len() > 0]\n",
    "test_df = test_df[test_df.norm_text.str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode (e deve!) rodar as duas células acima algumas vezes para checar os resultados da normalização. Caso não se sinta satisfeito com a normalização feita, volte e edite as _stopwords_ ou sua função de normalização até se sentir satisfeito(a) com os resultados :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo (finalmente!!!)\n",
    "\n",
    "Finalmente, chegamos à parte de treinar o modelo de análise de sentimentos!!!\n",
    "\n",
    "Ou quase... na verdade, antes de treinar o modelo, precisamos transformar o texto em _features_ numéricas.\n",
    "\n",
    "A maneira mais simples de transformar um texto em um vetor de números é usando o método comumente chamado de _Bag of words_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_for_bow = [\n",
    "    'camisa preta',\n",
    "    'botao feito linha preta',\n",
    "    'considera-se caro preco botao camisa botao',\n",
    "    'linha costurar botão mesma camisa',\n",
    "    'costurar linha camisa mesma botao'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5, strip_accents='unicode', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 15 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix = cv.fit_transform(examples_for_bow)\n",
    "bow_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camisa': 1, 'botao': 0, 'linha': 3, 'costurar': 2, 'mesma': 4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(botao, 0)</th>\n",
       "      <th>(camisa, 1)</th>\n",
       "      <th>(costurar, 2)</th>\n",
       "      <th>(linha, 3)</th>\n",
       "      <th>(mesma, 4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (botao, 0)  (camisa, 1)  (costurar, 2)  (linha, 3)  (mesma, 4)\n",
       "0           0            1              0           0           0\n",
       "1           1            0              0           1           0\n",
       "2           1            1              0           0           0\n",
       "3           1            1              1           1           1\n",
       "4           1            1              1           1           1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bow_matrix.todense(), columns=sorted(cv.vocabulary_.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que os exemplos `3` e `4` têm a mesma representação numérica, mesmo que a ordem das palavras não seja a mesma! Essa é uma característica desse método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treino de classificador _Naive Bayes_\n",
    "\n",
    "**Tarefa:** Monte um pipeline (criando um objeto do tipo `Pipeline`), que inclua a transformação do texto em _features_ numéricas (`CountVectorizer`) e o classificador `MultinomialNB`. Treine o modelo usando esse pipeline com os dados de input de `train_df`.\n",
    "\n",
    "Dica: Lembre-se de transformar o _target_ (coluna `polarity`) em 0s e 1s - 0 para `negative` e 1 para `positive`. Se quiser um exemplo, você pode ver esse [post no Medium](https://medium.com/@minbaekim/text-mining-preprocess-and-naive-bayes-classifier-da0000f633b2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ### complete\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ### complete\n",
    "y_train = ### complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa:** Teste seu classificador usando um texto de exemplo. Veja que você pode usar tanto o método `predict` como o método `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.predict(### complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.predict_proba(### complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avaliação do classificador\n",
    "\n",
    "**Tarefa:** Faça a predição da coluna `norm_text` e compare o resultado com o vetor target (coluna `polarity` em 0s e 1s).\n",
    "\n",
    "Dica: Imprima o [classification_report](https://scikit-learn.org/0.19/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) e a [matriz de confusão](https://scikit-learn.org/0.19/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ### complete\n",
    "y_test = ### complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sentiment_analyzer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** O que você achou do classificador? Ele é bom ou ruim?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mais considerações e possíveis modificações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que após aplicar o _Bag of words_, também poderíamos ter aplicado a transformação Tfidf, que discrimina as palavras de acordo com a \"relevância\" delas em cada documento em relação ao _corpus_ (i.e. o conjunto total de documentos).\n",
    "\n",
    "**Tarefa Bônus:** Inclua no _pipeline_ a transformação `Tfidf` e compare os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos que nosso classificador possui tanto o método `predict` como o método `predict_proba`. Ao usar o método `predict_proba`, temos as probabilidades de que o texto seja `positive` ou `negative`. Podemos escolher um _threshold_ de forma a maximizar uma das métricas.\n",
    "\n",
    "**Tarefa Bônus:** Escolha esse threshold de forma a maximizar nosso `recall` de exemplos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra alternativa seria processar o texto de uma maneira diferente: imagine que só quiséssemos incluir três tipos de classes gramaticais: verbo, adjetivo e advérbio, que normalmente são as classes que indicam o sentimento de um indivíduo em relação a alguma coisa.\n",
    "\n",
    "No modelo do `spacy`, é possível identificar os seguintes tipos de PoS tag:\n",
    "\n",
    "* ADJ\n",
    "* ADP\n",
    "* ADV\n",
    "* CONJ\n",
    "* DET\n",
    "* INTJ\n",
    "* NOUN\n",
    "* PART (e.g. possessive marker _'s_)\n",
    "* PRON\n",
    "* PROPN\n",
    "* PUNCT\n",
    "* SPACE\n",
    "* SYM\n",
    "* VERB\n",
    "* X (unknown)\n",
    "\n",
    "**Obs.:** na verdade isso vale para o _coarse-grained PoS tag_ (que é visto chamando o atributo `pos` de um objeto `Token`), para o _fine-grained PoS tag_, há muito [mais classificações disponíveis](https://spacy.io/api/annotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"the food is always fresh ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, se imprimíssemos somente as palavras de classe `ADJ`, `ADV` e `VERB`, teríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_pos = set(['ADJ', 'ADV', 'VERB'])\n",
    "for token in doc:\n",
    "    if token.pos_ not in allowed_pos:\n",
    "        continue\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tarefa Bônus:** Filtre os adjetivos, advérbios e verbos do texto e construa um novo modelo que use esse novo texto como input e avalie o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
